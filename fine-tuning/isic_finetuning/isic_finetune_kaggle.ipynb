{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# ISIC 2024 Skin Cancer Detection - MedGemma QLoRA Fine-Tuning\n",
    "\n",
    "Fine-tune **MedGemma 1.5 4B IT** for binary skin lesion classification (benign/malignant).\n",
    "\n",
    "**Data:** `train-metadata.csv` (401K rows) + `train-image.hdf5` (JPEG byte strings)\n",
    "\n",
    "**3-Layer Class Imbalance Strategy** (from KerasCV starter):\n",
    "- Layer 1: Sampling — downsample benign 1%, oversample malignant 5x\n",
    "- Layer 2: sklearn class weights — `compute_class_weight('balanced')`\n",
    "- Layer 3: Focal loss — gamma=2.0, alpha=0.75, per-sample weighted\n",
    "\n",
    "**Features:**\n",
    "- 4-bit QLoRA on Kaggle T4 GPU (16GB VRAM)\n",
    "- Tabular features as text in prompt (age, sex, site, size, DNN confidence)\n",
    "- StratifiedGroupKFold by patient_id (no data leakage)\n",
    "- Frozen SigLIP vision encoder\n",
    "- Structured JSON output with confidence scores\n",
    "\n",
    "**Requirements:** Attach `isic-2024-challenge` dataset and enable GPU T4 accelerator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U transformers accelerate peft bitsandbytes datasets huggingface_hub\n",
    "!pip install trl scikit-learn matplotlib tqdm torchvision h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Setup & Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, io, json, re, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import h5py\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, balanced_accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "# Auth\n",
    "from huggingface_hub import login\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    HF_TOKEN = UserSecretsClient().get_secret(\"HF_TOKEN\")\n",
    "except:\n",
    "    HF_TOKEN = os.getenv(\"HF_TOKEN\") or input(\"HF token: \")\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PATHS — auto-detect Kaggle vs local ===\n",
    "KAGGLE_BASE = \"/kaggle/input/isic-2024-challenge\"\n",
    "LOCAL_BASE = os.path.join(os.path.expanduser(\"~\"), \"medagents\", \"isic-2024-challenge\")\n",
    "\n",
    "IS_KAGGLE = os.path.exists(KAGGLE_BASE)\n",
    "DATA_DIR = KAGGLE_BASE if IS_KAGGLE else LOCAL_BASE\n",
    "\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train-metadata.csv\")\n",
    "TRAIN_HDF5 = os.path.join(DATA_DIR, \"train-image.hdf5\")\n",
    "TEST_CSV = os.path.join(DATA_DIR, \"test-metadata.csv\")\n",
    "TEST_HDF5 = os.path.join(DATA_DIR, \"test-image.hdf5\")\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/isic-medgemma\" if IS_KAGGLE else \"./output\"\n",
    "IMAGES_DIR = os.path.join(OUTPUT_DIR, \"images\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGES_DIR, exist_ok=True)\n",
    "\n",
    "# === MODEL ===\n",
    "MODEL_NAME = \"google/medgemma-1.5-4b-it\"\n",
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "# === QLoRA ===\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "# === TRAINING ===\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION = 16\n",
    "LEARNING_RATE = 2e-4\n",
    "MAX_STEPS = 500\n",
    "WARMUP_STEPS = 50\n",
    "\n",
    "# === CLASS IMBALANCE (from KerasCV starter) ===\n",
    "NEG_SAMPLE_FRAC = 0.01    # Layer 1: downsample benign to 1%\n",
    "POS_SAMPLE_FRAC = 5.0     # Layer 1: oversample malignant 5x\n",
    "FOCAL_GAMMA = 2.0         # Layer 3: focal loss gamma\n",
    "FOCAL_ALPHA = 0.75        # Layer 3: focal loss alpha\n",
    "\n",
    "# === SPLIT ===\n",
    "N_FOLDS = 5\n",
    "VAL_FOLD = 0\n",
    "\n",
    "# === PROMPT TEMPLATE ===\n",
    "METADATA_TEMPLATE = (\n",
    "    \"Patient: {age} year old {sex}\\n\"\n",
    "    \"Lesion site: {site}, size: {size_mm} mm\\n\"\n",
    "    \"DNN confidence: {dnn_confidence}, Nevi confidence: {nevi_confidence}\\n\"\n",
    ")\n",
    "\n",
    "print(f\"Data: {DATA_DIR}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "print(f\"Kaggle: {IS_KAGGLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Load Data & Class Balancing (Layer 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train-metadata.csv\n",
    "df = pd.read_csv(TRAIN_CSV)\n",
    "print(f\"Loaded: {len(df)} rows, {df.shape[1]} columns\")\n",
    "print(f\"Benign: {(df['target']==0).sum()}, Malignant: {(df['target']==1).sum()}\")\n",
    "print(f\"Malignant rate: {df['target'].mean()*100:.2f}%\")\n",
    "\n",
    "# Layer 1: Sampling (from KerasCV starter)\n",
    "benign = df[df['target'] == 0]\n",
    "malignant = df[df['target'] == 1]\n",
    "\n",
    "benign_sampled = benign.sample(frac=NEG_SAMPLE_FRAC, random_state=SEED)\n",
    "malignant_sampled = malignant.sample(frac=POS_SAMPLE_FRAC, replace=True, random_state=SEED)\n",
    "\n",
    "sampled = pd.concat([benign_sampled, malignant_sampled]).sample(\n",
    "    frac=1.0, random_state=SEED\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nLayer 1 - Sampling:\")\n",
    "print(f\"  Benign:    {len(benign)} -> {len(benign_sampled)} (frac={NEG_SAMPLE_FRAC})\")\n",
    "print(f\"  Malignant: {len(malignant)} -> {len(malignant_sampled)} (frac={POS_SAMPLE_FRAC})\")\n",
    "print(f\"  Total:     {len(sampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 2: Compute sklearn class weights\n",
    "y = sampled['target'].values\n",
    "classes = np.unique(y)\n",
    "weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "class_weights = {int(c): float(w) for c, w in zip(classes, weights)}\n",
    "\n",
    "print(f\"Layer 2 - Class weights:\")\n",
    "print(f\"  Benign (0):    {class_weights[0]:.4f}\")\n",
    "print(f\"  Malignant (1): {class_weights[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Train/Val Split (StratifiedGroupKFold by patient_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing patient_id with isic_id\n",
    "sampled['patient_id'] = sampled['patient_id'].fillna(sampled['isic_id'])\n",
    "\n",
    "sgkf = StratifiedGroupKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "splits = list(sgkf.split(sampled, sampled['target'], groups=sampled['patient_id']))\n",
    "\n",
    "train_idx, val_idx = splits[VAL_FOLD]\n",
    "train_df = sampled.iloc[train_idx].reset_index(drop=True)\n",
    "val_df = sampled.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "# Verify no patient leakage\n",
    "train_patients = set(train_df['patient_id'].unique())\n",
    "val_patients = set(val_df['patient_id'].unique())\n",
    "overlap = train_patients & val_patients\n",
    "\n",
    "print(f\"Train: {len(train_df)} (mal: {(train_df['target']==1).sum()}, ben: {(train_df['target']==0).sum()})\")\n",
    "print(f\"Val:   {len(val_df)} (mal: {(val_df['target']==1).sum()}, ben: {(val_df['target']==0).sum()})\")\n",
    "print(f\"Patient leakage: {'YES - ' + str(len(overlap)) + ' patients!' if overlap else 'None (verified)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Extract Images from HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all unique isic_ids needed\n",
    "all_ids = set(train_df['isic_id'].unique()) | set(val_df['isic_id'].unique())\n",
    "print(f\"Need {len(all_ids)} unique images\")\n",
    "\n",
    "hdf5 = h5py.File(TRAIN_HDF5, \"r\")\n",
    "extracted = 0\n",
    "failed = 0\n",
    "\n",
    "for isic_id in tqdm(all_ids, desc=\"Extracting images\"):\n",
    "    out_path = os.path.join(IMAGES_DIR, f\"{isic_id}.jpg\")\n",
    "    if os.path.exists(out_path):\n",
    "        extracted += 1\n",
    "        continue\n",
    "    try:\n",
    "        byte_string = hdf5[isic_id][()]\n",
    "        img = Image.open(io.BytesIO(byte_string)).convert(\"RGB\")\n",
    "        img.save(out_path, \"JPEG\", quality=95)\n",
    "        extracted += 1\n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "\n",
    "hdf5.close()\n",
    "print(f\"Extracted: {extracted}, Failed: {failed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Build Prompts & Format Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_metadata(row):\n",
    "    \"\"\"Format tabular features into prompt text.\"\"\"\n",
    "    age = row.get('age_approx')\n",
    "    sex = row.get('sex', 'unknown')\n",
    "    site = row.get('anatom_site_general', 'unknown')\n",
    "    size_mm = row.get('clin_size_long_diam_mm')\n",
    "    dnn_val = row.get('tbp_lv_dnn_lesion_confidence')\n",
    "    nevi_val = row.get('tbp_lv_nevi_confidence')\n",
    "    \n",
    "    return METADATA_TEMPLATE.format(\n",
    "        age=str(int(age)) if pd.notna(age) else 'unknown',\n",
    "        sex=str(sex) if pd.notna(sex) else 'unknown',\n",
    "        site=str(site) if pd.notna(site) else 'unknown',\n",
    "        size_mm=f'{size_mm:.1f}' if pd.notna(size_mm) else 'unknown',\n",
    "        dnn_confidence=f'{dnn_val:.3f}' if pd.notna(dnn_val) else 'N/A',\n",
    "        nevi_confidence=f'{nevi_val:.3f}' if pd.notna(nevi_val) else 'N/A',\n",
    "    )\n",
    "\n",
    "\n",
    "def make_response(is_malignant):\n",
    "    \"\"\"Generate training response JSON.\"\"\"\n",
    "    if is_malignant:\n",
    "        conf = round(random.uniform(0.75, 0.95), 2)\n",
    "        reasons = [\n",
    "            \"Irregular morphology with asymmetric features suggesting potential malignancy.\",\n",
    "            \"Atypical dermoscopic pattern with concerning structural features.\",\n",
    "            \"Lesion shows irregular pigment distribution and border irregularity.\",\n",
    "            \"Morphological features are atypical, warranting biopsy and further evaluation.\",\n",
    "            \"Heterogeneous pattern with multiple dermoscopic criteria for malignancy.\",\n",
    "        ]\n",
    "    else:\n",
    "        conf = round(random.uniform(0.80, 0.99), 2)\n",
    "        reasons = [\n",
    "            \"Regular symmetric pattern consistent with benign melanocytic proliferation.\",\n",
    "            \"Typical dermoscopic features of a benign nevus with uniform pigmentation.\",\n",
    "            \"Homogeneous pattern with regular borders consistent with benign lesion.\",\n",
    "            \"Symmetric structure with regular pigment network, no concerning features.\",\n",
    "            \"Benign morphology with uniform color and well-defined borders.\",\n",
    "        ]\n",
    "    return json.dumps({\n",
    "        \"classification\": \"malignant\" if is_malignant else \"benign\",\n",
    "        \"confidence\": conf,\n",
    "        \"reasoning\": random.choice(reasons),\n",
    "    })\n",
    "\n",
    "\n",
    "def row_to_conversation(row):\n",
    "    \"\"\"Convert a row to VLM conversation dict.\"\"\"\n",
    "    meta = format_metadata(row)\n",
    "    user_text = (\n",
    "        \"Analyze this dermoscopic skin lesion image for signs of malignancy.\\n\"\n",
    "        f\"{meta}\"\n",
    "        'Provide your assessment as JSON: '\n",
    "        '{\"classification\": \"benign\" or \"malignant\", '\n",
    "        '\"confidence\": 0.0-1.0, '\n",
    "        '\"reasoning\": \"brief explanation\"}'\n",
    "    )\n",
    "    is_mal = row['target'] == 1\n",
    "    image_path = os.path.join(IMAGES_DIR, f\"{row['isic_id']}.jpg\")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": user_text},\n",
    "            ]},\n",
    "            {\"role\": \"assistant\", \"content\": [\n",
    "                {\"type\": \"text\", \"text\": make_response(is_mal)},\n",
    "            ]},\n",
    "        ],\n",
    "        \"image_path\": image_path,\n",
    "        \"isic_id\": row['isic_id'],\n",
    "        \"malignant\": int(is_mal),\n",
    "    }\n",
    "\n",
    "\n",
    "# Build conversations (skip samples with missing images)\n",
    "train_convos = []\n",
    "for _, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Train convos\"):\n",
    "    img_path = os.path.join(IMAGES_DIR, f\"{row['isic_id']}.jpg\")\n",
    "    if os.path.exists(img_path):\n",
    "        train_convos.append(row_to_conversation(row))\n",
    "\n",
    "val_convos = []\n",
    "for _, row in tqdm(val_df.iterrows(), total=len(val_df), desc=\"Val convos\"):\n",
    "    img_path = os.path.join(IMAGES_DIR, f\"{row['isic_id']}.jpg\")\n",
    "    if os.path.exists(img_path):\n",
    "        val_convos.append(row_to_conversation(row))\n",
    "\n",
    "print(f\"Conversations: train={len(train_convos)}, val={len(val_convos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8. Load MedGemma with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "clear_memory()\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading {MODEL_NAME}...\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN, trust_remote_code=True)\n",
    "\n",
    "if processor.tokenizer.pad_token is None:\n",
    "    processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "    model.config.pad_token_id = processor.tokenizer.eos_token_id\n",
    "\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for QLoRA\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Freeze vision encoder\n",
    "frozen = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if \"vision\" in name.lower() or \"siglip\" in name.lower() or \"image_encoder\" in name.lower():\n",
    "        param.requires_grad = False\n",
    "        frozen += param.numel()\n",
    "print(f\"Froze vision encoder: {frozen:,} params\")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"LoRA: {trainable:,} trainable / {total:,} total ({100*trainable/total:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## 9. Dataset & Training Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, conversations, processor, max_length=MAX_SEQ_LENGTH):\n",
    "        self.data = conversations\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
    "        \n",
    "        # Tokenize conversation\n",
    "        text = self.processor.apply_chat_template(\n",
    "            sample[\"messages\"], tokenize=False, add_generation_prompt=False,\n",
    "        )\n",
    "        inputs = self.processor(\n",
    "            text=text, images=[image], return_tensors=\"pt\",\n",
    "            padding=\"max_length\", max_length=self.max_length, truncation=True,\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)\n",
    "        attention_mask = inputs[\"attention_mask\"].squeeze(0)\n",
    "        pixel_values = inputs.get(\"pixel_values\")\n",
    "        if pixel_values is not None:\n",
    "            pixel_values = pixel_values.squeeze(0)\n",
    "        \n",
    "        # Create labels (mask non-response tokens)\n",
    "        labels = input_ids.clone()\n",
    "        pad_id = self.processor.tokenizer.pad_token_id\n",
    "        if pad_id is not None:\n",
    "            labels[input_ids == pad_id] = -100\n",
    "        \n",
    "        # Find model turn and mask everything before it\n",
    "        marker = self.processor.tokenizer.encode(\"<start_of_turn>model\\n\", add_special_tokens=False)\n",
    "        token_list = input_ids.tolist()\n",
    "        for i in range(len(token_list) - len(marker) + 1):\n",
    "            if token_list[i:i+len(marker)] == marker:\n",
    "                labels[:i+len(marker)] = -100\n",
    "                break\n",
    "        \n",
    "        result = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "            \"is_malignant\": sample[\"malignant\"],\n",
    "        }\n",
    "        if pixel_values is not None:\n",
    "            result[\"pixel_values\"] = pixel_values\n",
    "        return result\n",
    "\n",
    "\n",
    "class ISICCollator:\n",
    "    def __call__(self, features):\n",
    "        batch = {\n",
    "            \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n",
    "            \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n",
    "            \"labels\": torch.stack([f[\"labels\"] for f in features]),\n",
    "            \"is_malignant\": torch.tensor([f[\"is_malignant\"] for f in features], dtype=torch.float32),\n",
    "        }\n",
    "        if \"pixel_values\" in features[0] and features[0][\"pixel_values\"] is not None:\n",
    "            batch[\"pixel_values\"] = torch.stack([f[\"pixel_values\"] for f in features])\n",
    "        return batch\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Layer 3: Focal loss with per-sample class weights.\"\"\"\n",
    "    def __init__(self, gamma=FOCAL_GAMMA, alpha=FOCAL_ALPHA):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, logits, labels, sample_weights=None):\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        flat_logits = shift_logits.view(-1, shift_logits.shape[-1])\n",
    "        flat_labels = shift_labels.view(-1)\n",
    "        \n",
    "        valid = flat_labels != -100\n",
    "        if not valid.any():\n",
    "            return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
    "        \n",
    "        ce = F.cross_entropy(flat_logits[valid], flat_labels[valid], reduction=\"none\")\n",
    "        pt = torch.exp(-ce)\n",
    "        focal = self.alpha * (1 - pt) ** self.gamma * ce\n",
    "        \n",
    "        if sample_weights is not None:\n",
    "            seq_len = shift_labels.shape[1]\n",
    "            w = sample_weights.unsqueeze(1).expand(-1, seq_len).contiguous().view(-1)\n",
    "            focal = focal * w[valid]\n",
    "        \n",
    "        return focal.mean()\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "class ISICTrainer(Trainer):\n",
    "    \"\"\"Custom trainer with focal loss + sklearn class weights.\"\"\"\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.focal_loss = FocalLoss()\n",
    "        self.class_weights = class_weights or {0: 1.0, 1: 1.0}\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        is_mal = inputs.pop(\"is_malignant\", None)\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Layer 2: Per-sample weights from sklearn\n",
    "        weights = None\n",
    "        if is_mal is not None:\n",
    "            weights = torch.where(is_mal.bool(),\n",
    "                torch.tensor(self.class_weights.get(1, 1.0), device=outputs.logits.device),\n",
    "                torch.tensor(self.class_weights.get(0, 1.0), device=outputs.logits.device))\n",
    "        \n",
    "        # Layer 3: Focal loss\n",
    "        loss = self.focal_loss(outputs.logits, inputs[\"labels\"], weights)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "print(\"Classes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 10. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ISICDataset(train_convos, processor)\n",
    "val_ds = ISICDataset(val_convos, processor)\n",
    "collator = ISICCollator()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    max_steps=MAX_STEPS,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    logging_steps=10,\n",
    "    eval_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    "    dataloader_pin_memory=False,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = ISICTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collator,\n",
    "    class_weights=class_weights,\n",
    ")\n",
    "\n",
    "print(f\"Training: {MAX_STEPS} steps, effective batch={BATCH_SIZE*GRADIENT_ACCUMULATION}\")\n",
    "print(f\"Layer 2 weights: benign={class_weights[0]:.4f}, malignant={class_weights[1]:.4f}\")\n",
    "print(f\"Layer 3 focal: gamma={FOCAL_GAMMA}, alpha={FOCAL_ALPHA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_memory()\n",
    "print(\"Training started...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 11. Evaluate on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_output(text):\n",
    "    \"\"\"Parse model JSON output with regex fallback.\"\"\"\n",
    "    try:\n",
    "        m = re.search(r'\\{[^{}]*\\}', text)\n",
    "        if m:\n",
    "            parsed = json.loads(m.group())\n",
    "            if 'classification' in parsed and 'confidence' in parsed:\n",
    "                return parsed\n",
    "    except:\n",
    "        pass\n",
    "    result = {\"classification\": \"benign\", \"confidence\": 0.5}\n",
    "    cm = re.search(r'\"classification\"\\s*:\\s*\"(malignant|benign)\"', text, re.I)\n",
    "    if cm: result[\"classification\"] = cm.group(1).lower()\n",
    "    cf = re.search(r'\"confidence\"\\s*:\\s*(0?\\.\\d+|1\\.0|1)', text)\n",
    "    if cf: result[\"confidence\"] = float(cf.group(1))\n",
    "    return result\n",
    "\n",
    "# Run inference on validation set\n",
    "model.eval()\n",
    "predictions = []\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "for sample in tqdm(val_convos, desc=\"Evaluating\"):\n",
    "    image = Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
    "    msgs = [sample[\"messages\"][0]]  # user only\n",
    "    \n",
    "    text = processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = processor(text=text, images=[image], return_tensors=\"pt\",\n",
    "                       padding=True, max_length=MAX_SEQ_LENGTH, truncation=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=150, temperature=0.1,\n",
    "                             do_sample=False, pad_token_id=processor.tokenizer.eos_token_id)\n",
    "    \n",
    "    gen = out[0][inputs[\"input_ids\"].shape[1]:]\n",
    "    resp = processor.tokenizer.decode(gen, skip_special_tokens=True)\n",
    "    parsed = parse_output(resp)\n",
    "    \n",
    "    predictions.append({\n",
    "        \"isic_id\": sample[\"isic_id\"],\n",
    "        \"true_label\": sample[\"malignant\"],\n",
    "        \"pred_class\": parsed[\"classification\"],\n",
    "        \"pred_conf\": parsed[\"confidence\"],\n",
    "    })\n",
    "\n",
    "pred_df = pd.DataFrame(predictions)\n",
    "print(f\"Predictions: {len(pred_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "y_true = pred_df[\"true_label\"].values.astype(int)\n",
    "y_pred = (pred_df[\"pred_class\"] == \"malignant\").astype(int).values\n",
    "y_scores = np.where(pred_df[\"pred_class\"] == \"malignant\", pred_df[\"pred_conf\"], 1 - pred_df[\"pred_conf\"])\n",
    "\n",
    "# pAUC (ISIC 2024 competition metric)\n",
    "fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "mask = tpr >= 0.80\n",
    "if mask.any() and len(fpr[mask]) >= 2:\n",
    "    pauc_raw = np.trapz(tpr[mask], fpr[mask])\n",
    "    # Rescale pAUC\n",
    "    max_fpr_range = fpr[mask][-1] - fpr[mask][0]\n",
    "    if max_fpr_range > 0:\n",
    "        min_pauc = 0.5 * max_fpr_range * (1 + 0.80)\n",
    "        max_pauc = max_fpr_range * 1.0\n",
    "        pauc = max(0.0, min(1.0, (pauc_raw - min_pauc) / (max_pauc - min_pauc))) if max_pauc > min_pauc else 0.0\n",
    "    else:\n",
    "        pauc = 0.0\n",
    "else:\n",
    "    pauc = 0.0\n",
    "\n",
    "auc = roc_auc_score(y_true, y_scores) if len(np.unique(y_true)) > 1 else 0.0\n",
    "tp = ((y_pred == 1) & (y_true == 1)).sum()\n",
    "fn = ((y_pred == 0) & (y_true == 1)).sum()\n",
    "tn = ((y_pred == 0) & (y_true == 0)).sum()\n",
    "fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "\n",
    "sens = tp / max(tp + fn, 1)\n",
    "spec = tn / max(tn + fp, 1)\n",
    "bal_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"AUC-ROC:          {auc:.4f}\")\n",
    "print(f\"pAUC (TPR>=0.80): {pauc:.4f}\")\n",
    "print(f\"Sensitivity:      {sens:.4f}\")\n",
    "print(f\"Specificity:      {spec:.4f}\")\n",
    "print(f\"Balanced Acc:     {bal_acc:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "print(f\"  True Benign:    {cm[0][0]:>5d} correct, {cm[0][1]:>5d} false alarm\")\n",
    "print(f\"  True Malignant: {cm[1][1]:>5d} detected, {cm[1][0]:>5d} missed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve + Confusion Matrix plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(fpr, tpr, 'b-', lw=2, label=f'ROC (AUC={auc:.4f})')\n",
    "axes[0].plot([0,1], [0,1], 'k--', alpha=0.5)\n",
    "if mask.any():\n",
    "    axes[0].fill_between(fpr[mask], 0.8, tpr[mask], alpha=0.2, color='green', label='pAUC region')\n",
    "axes[0].axhline(y=0.8, color='r', linestyle=':', alpha=0.5)\n",
    "axes[0].set_xlabel('FPR'); axes[0].set_ylabel('TPR')\n",
    "axes[0].set_title('ROC Curve'); axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "im = axes[1].imshow(cm, cmap='Blues')\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        c = 'white' if cm[i,j] > cm.max()/2 else 'black'\n",
    "        axes[1].text(j, i, str(cm[i,j]), ha='center', va='center', color=c, fontsize=14)\n",
    "axes[1].set_xticks([0,1]); axes[1].set_yticks([0,1])\n",
    "axes[1].set_xticklabels(['Benign','Malignant']); axes[1].set_yticklabels(['Benign','Malignant'])\n",
    "axes[1].set_xlabel('Predicted'); axes[1].set_ylabel('True')\n",
    "axes[1].set_title('Confusion Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'eval_plots.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 12. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapter\n",
    "adapter_dir = os.path.join(OUTPUT_DIR, \"lora_adapter\")\n",
    "model.save_pretrained(adapter_dir)\n",
    "processor.save_pretrained(adapter_dir)\n",
    "\n",
    "# Save config + metrics\n",
    "config = {\n",
    "    \"model\": MODEL_NAME, \"lora_r\": LORA_R, \"lora_alpha\": LORA_ALPHA,\n",
    "    \"focal_gamma\": FOCAL_GAMMA, \"focal_alpha\": FOCAL_ALPHA,\n",
    "    \"class_weights\": class_weights,\n",
    "    \"neg_sample_frac\": NEG_SAMPLE_FRAC, \"pos_sample_frac\": POS_SAMPLE_FRAC,\n",
    "    \"max_steps\": MAX_STEPS, \"n_folds\": N_FOLDS, \"val_fold\": VAL_FOLD,\n",
    "    \"auc_roc\": float(auc), \"pauc\": float(pauc),\n",
    "    \"sensitivity\": float(sens), \"specificity\": float(spec),\n",
    "}\n",
    "with open(os.path.join(adapter_dir, \"training_config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# Save predictions\n",
    "pred_df.to_csv(os.path.join(OUTPUT_DIR, \"predictions.csv\"), index=False)\n",
    "\n",
    "# Zip for download\n",
    "import subprocess\n",
    "subprocess.run([\"zip\", \"-r\", os.path.join(OUTPUT_DIR, \"lora_adapter.zip\"),\n",
    "                adapter_dir], capture_output=True)\n",
    "\n",
    "print(f\"Saved to: {adapter_dir}\")\n",
    "print(f\"Download: {OUTPUT_DIR}/lora_adapter.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82almnuf3v4",
   "source": "## 13. Merge LoRA into Base Model (for GGUF Export)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vaib50sg01e",
   "source": "# Free the 4-bit training model to make room for float16 merge\ndel model, trainer\nclear_memory()\n\nfrom transformers import AutoModelForImageTextToText, AutoProcessor\nfrom peft import PeftModel\n\n# Load base model in float16 (NOT 4-bit — need full precision for clean merge)\nprint(\"Loading base model in float16 for LoRA merge...\")\nbase_model = AutoModelForImageTextToText.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    token=HF_TOKEN,\n    trust_remote_code=True,\n)\nmerge_processor = AutoProcessor.from_pretrained(MODEL_NAME, token=HF_TOKEN, trust_remote_code=True)\n\n# Load and merge LoRA adapter\nadapter_dir = os.path.join(OUTPUT_DIR, \"lora_adapter\")\nprint(f\"Loading LoRA adapter from {adapter_dir}...\")\nmerged_model = PeftModel.from_pretrained(base_model, adapter_dir)\nmerged_model = merged_model.merge_and_unload()\nprint(\"LoRA merged into base model!\")\n\n# Save merged model as safetensors\nmerged_dir = os.path.join(OUTPUT_DIR, \"merged_model\")\nos.makedirs(merged_dir, exist_ok=True)\nmerged_model.save_pretrained(merged_dir, safe_serialization=True)\nmerge_processor.save_pretrained(merged_dir)\n\n# Free GPU memory\ndel merged_model, base_model\nclear_memory()\n\nprint(f\"Merged model saved to {merged_dir}\")\n!du -sh {merged_dir}",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "uw17b8aeyne",
   "source": "## 14. Convert to GGUF (for Ollama / llama.cpp CPU Serving)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "98cl3wwueie",
   "source": "%%bash -e\n# Clone llama.cpp (shallow clone for speed) and install conversion deps\necho \"=== Cloning llama.cpp ===\"\ngit clone --depth 1 https://github.com/ggml-org/llama.cpp /kaggle/working/llama.cpp 2>&1 | tail -3\n\necho \"=== Installing conversion requirements ===\"\npip install -q -r /kaggle/working/llama.cpp/requirements/requirements-convert_hf_to_gguf.txt 2>&1 | tail -3\n\necho \"=== Building llama-quantize ===\"\ncd /kaggle/working/llama.cpp\ncmake -B build -DCMAKE_BUILD_TYPE=Release -DGGML_CUDA=OFF 2>&1 | tail -3\ncmake --build build --config Release -j$(nproc) --target llama-quantize 2>&1 | tail -3\n\necho \"=== Done ===\"\nls -la build/bin/llama-quantize",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "xytab2w51",
   "source": "import subprocess\n\nmerged_dir = os.path.join(OUTPUT_DIR, \"merged_model\")\ngguf_dir = os.path.join(OUTPUT_DIR, \"gguf\")\nos.makedirs(gguf_dir, exist_ok=True)\n\nf16_gguf = os.path.join(gguf_dir, \"medgemma-isic-f16.gguf\")\nmmproj_gguf = os.path.join(gguf_dir, \"mmproj-medgemma-isic-f16.gguf\")\nq4_gguf = os.path.join(gguf_dir, \"medgemma-isic-Q4_K_M.gguf\")\n\n# Step 1: Convert text model to GGUF F16\nprint(\"=\" * 50)\nprint(\"Step 1/3: Converting text model to GGUF F16...\")\nprint(\"=\" * 50)\nsubprocess.run([\n    \"python\", \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\",\n    merged_dir, \"--outfile\", f16_gguf, \"--outtype\", \"f16\"\n], check=True)\n\n# Step 2: Convert mmproj (vision encoder + projector) to GGUF F16\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Step 2/3: Converting vision encoder (mmproj) to GGUF F16...\")\nprint(\"=\" * 50)\nsubprocess.run([\n    \"python\", \"/kaggle/working/llama.cpp/convert_hf_to_gguf.py\",\n    merged_dir, \"--mmproj\", \"--outfile\", mmproj_gguf, \"--outtype\", \"f16\"\n], check=True)\n\n# Step 3: Quantize text model to Q4_K_M for CPU efficiency\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Step 3/3: Quantizing text model to Q4_K_M...\")\nprint(\"=\" * 50)\nsubprocess.run([\n    \"/kaggle/working/llama.cpp/build/bin/llama-quantize\",\n    f16_gguf, q4_gguf, \"Q4_K_M\"\n], check=True)\n\n# Delete the large F16 text model (no longer needed)\nos.remove(f16_gguf)\nprint(f\"\\nDeleted intermediate F16 model to save space.\")\n\n# Print final sizes\nprint(\"\\n\" + \"=\" * 50)\nprint(\"GGUF FILES READY\")\nprint(\"=\" * 50)\nfor f in [q4_gguf, mmproj_gguf]:\n    size_mb = os.path.getsize(f) / (1024 * 1024)\n    print(f\"  {os.path.basename(f)}: {size_mb:.0f} MB\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "wdaae8cf92g",
   "source": "## 15. Package GGUF for Download & Ollama Setup",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "m5tazdbkwzr",
   "source": "import subprocess\n\ngguf_dir = os.path.join(OUTPUT_DIR, \"gguf\")\nq4_gguf = os.path.join(gguf_dir, \"medgemma-isic-Q4_K_M.gguf\")\nmmproj_gguf = os.path.join(gguf_dir, \"mmproj-medgemma-isic-f16.gguf\")\n\n# Create Ollama Modelfile\nmodelfile_content = \"\"\"FROM ./medgemma-isic-Q4_K_M.gguf\nPROJECTOR ./mmproj-medgemma-isic-f16.gguf\n\nSYSTEM \\\"\\\"\\\"You are a dermatology AI assistant specialized in skin lesion analysis. Analyze dermoscopic images and classify lesions as benign or malignant with a confidence score.\\\"\\\"\\\"\n\nPARAMETER temperature 0.1\nPARAMETER top_p 0.9\nPARAMETER num_predict 200\nPARAMETER stop <end_of_turn>\n\"\"\"\n\nmodelfile_path = os.path.join(gguf_dir, \"Modelfile\")\nwith open(modelfile_path, \"w\") as f:\n    f.write(modelfile_content)\nprint(f\"Modelfile written to {modelfile_path}\")\n\n# Zip GGUF files + Modelfile for download\nzip_path = os.path.join(OUTPUT_DIR, \"medgemma-isic-gguf.zip\")\nsubprocess.run([\n    \"zip\", \"-j\", zip_path,\n    q4_gguf,\n    mmproj_gguf,\n    modelfile_path,\n], check=True)\n\nzip_size_mb = os.path.getsize(zip_path) / (1024 * 1024)\nprint(f\"\\n{'=' * 50}\")\nprint(f\"GGUF PACKAGE READY FOR DOWNLOAD\")\nprint(f\"{'=' * 50}\")\nprint(f\"  Archive: {zip_path} ({zip_size_mb:.0f} MB)\")\nprint(f\"\\n  Contents:\")\nprint(f\"    - medgemma-isic-Q4_K_M.gguf  (quantized text model)\")\nprint(f\"    - mmproj-medgemma-isic-f16.gguf  (vision encoder)\")\nprint(f\"    - Modelfile  (Ollama model definition)\")\nprint(f\"\\n  LOCAL SETUP INSTRUCTIONS:\")\nprint(f\"  1. Download medgemma-isic-gguf.zip from Kaggle output\")\nprint(f\"  2. unzip medgemma-isic-gguf.zip -d ~/models/isic-medgemma/\")\nprint(f\"  3. cd ~/models/isic-medgemma/\")\nprint(f\"  4. ollama create isic-medgemma -f Modelfile\")\nprint(f\"  5. ollama run isic-medgemma 'Analyze this skin lesion'\")\nprint(f\"\\n  Python API:\")\nprint(f\"    python isic_ollama_serve.py --image lesion.jpg\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}